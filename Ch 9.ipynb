{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import nltk, re, pprint\n",
    "from nltk import grammar, parse, load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'}\n",
      "{'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase', 'AGT': 'sbj', 'PAT': 'obj'}\n",
      "{'CAT': 'NP', 'ORTH': 'Lee', 'REF': 'l'}\n",
      "{'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase', 'AGT': 'k', 'PAT': 'l'}\n",
      "ORTH  => chased\n",
      "REL   => chase\n",
      "AGT   => k\n",
      "PAT   => l\n",
      "l\n"
     ]
    }
   ],
   "source": [
    "kim = {'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'}\n",
    "chase = {'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase'}\n",
    "sent = \"Kim chased Lee\"\n",
    "chase['AGT'] = 'sbj'\n",
    "chase['PAT'] = 'obj'\n",
    "tokens = sent.split()\n",
    "lee = {'CAT': 'NP', 'ORTH': 'Lee', 'REF': 'l'}\n",
    "def lex2fs(word):\n",
    "    for fs in [kim, lee, chase]:\n",
    "        if fs['ORTH'] == word:\n",
    "            print(fs)\n",
    "            return fs\n",
    "subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])\n",
    "\n",
    "verb['AGT'] = subj['REF']\n",
    "verb['PAT'] = obj['REF']\n",
    "print(chase)\n",
    "for k in ['ORTH', 'REL', 'AGT', 'PAT']:\n",
    "    print(\"%-5s => %s\" % (k, verb[k]))\n",
    "print(verb[k]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORTH => chased\n",
      "REL => chase\n",
      "AGT => k\n",
      "PAT => l\n"
     ]
    }
   ],
   "source": [
    "for k in ['ORTH', 'REL', 'AGT', 'PAT']:\n",
    "    print(\"%s => %s\" % (k, verb[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the % character informs python it will have to substitute something to a token\n",
    "#the s character informs python the token will be a string\n",
    "#the 5 (or whatever number you wish) informs python to pad the string with spaces up to 5 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What constraints are required to correctly parse word sequences like I am happy and she is happy but not *you is happy or *they am happy? Implement two solutions for the present tense paradigm of the verb be in English, first taking Grammar\n",
    "(8) as your starting point, and then taking Grammar (20) as the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "# S expansion productions\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# NP expansion productions\n",
      "NP[NUM=?n] -> N[NUM=?n] \n",
      "NP[NUM=?n] -> PropN[NUM=?n] \n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "NP[NUM=pl] -> N[NUM=pl] \n",
      "# VP expansion productions\n",
      "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg] -> 'this' | 'every'\n",
      "Det[NUM=pl] -> 'these' | 'all'\n",
      "Det -> 'the' | 'some' | 'several'\n",
      "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
      "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
      "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children' \n",
      "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
      "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
      "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
      "TV[TENSE=past] -> 'saw' | 'liked'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = load_parser('file:Cp9_1.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = 'I am happy'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = 'she is happy'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP_SG_1[] (Pro_SG_1[] I))\n",
      "  (VP_SG_1[] (Cop_SG_1[] am) (Adj[] happy)))\n"
     ]
    }
   ],
   "source": [
    "for tree in grammar.parse(tokens1):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP_SG_3[] (Pro_SG_3[] she))\n",
      "  (VP_SG_3[] (Cop_SG_3[] is) (Adj[] happy)))\n"
     ]
    }
   ],
   "source": [
    "for tree in grammar.parse(tokens2):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1]] (Pro[AGR=[NUM='sg', PER=1]] I))\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (Cop[AGR=[NUM='sg', PER=1]] am)\n",
      "    (Adj[] happy)))\n"
     ]
    }
   ],
   "source": [
    "grammar_a = load_parser('file:Cp9_1_a.fcfg')\n",
    "for tree in grammar_a.parse(tokens1):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=2]] (Pro[AGR=[NUM='sg', PER=2]] you))\n",
      "  (VP[AGR=[NUM='sg', PER=2]]\n",
      "    (Cop[AGR=[NUM='sg', PER=2]] are)\n",
      "    (Adj[] happy)))\n",
      "(S[]\n",
      "  (NP[AGR=[NUM='pl', PER=2]] (Pro[AGR=[NUM='pl', PER=2]] you))\n",
      "  (VP[AGR=[NUM='pl', PER=2]]\n",
      "    (Cop[AGR=[NUM='pl', PER=2]] are)\n",
      "    (Adj[] happy)))\n"
     ]
    }
   ],
   "source": [
    "tokens3 = 'you are happy'.split()\n",
    "for tree in grammar_a.parse(tokens3):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Develop a variant of grammar in Example 9-1 that uses a feature COUNT to make the distinctions shown here:\n",
    "(56) a. The boy sings.\n",
    "b. *Boy sings.\n",
    "(57) a. The boys sing.\n",
    "b. Boys sing.\n",
    "(58) a. The water is precious.\n",
    "b. Water is precious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT Nouns which can be made plural are +COUNT, e.g. dog. Nouns which are –COUNT include water (in its usual sense), mankind or sincerity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"The boy sings\".lower().split()\n",
    "sent2 = \"The boys sing\".lower().split()\n",
    "sent3 = \"Boys sing\".lower().split()\n",
    "sent4 = \"The water is precious\".lower().split()\n",
    "sent5 = \"Water is precious\".lower().split()\n",
    "sent6 = \"Boy sings\".lower().split()\n",
    "grammar_a = load_parser('file:Cp9_2a.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[NUM='sg'] (Det[] the) (N[NUM='sg', +count] boy))\n",
      "  (VP[NUM='sg', TENSE='pres'] (IV[NUM='sg', TENSE='pres'] sings)))\n"
     ]
    }
   ],
   "source": [
    "for tree in grammar_a.parse(sent1):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[NUM='pl'] (Det[] the) (N[NUM='pl', +count] boys))\n",
      "  (VP[NUM='pl', TENSE='pres'] (IV[NUM='pl', TENSE='pres'] sing)))\n"
     ]
    }
   ],
   "source": [
    "for tree in grammar_a.parse(sent2):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in grammar_a.parse(sent6):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function subsumes() that holds of two feature structures fs1 and fs2 just in case fs1 subsumes fs2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont understand the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Modify the grammar illustrated in (30) to incorporate a BAR feature for dealing with phrasal projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-51-56a03c099ea7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-51-56a03c099ea7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\n",
    "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\n",
    "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=clause, TENSE=?t, NUM=?n] SBar\n",
    "\n",
    "SBar -> Comp S\n",
    "Comp -> 'that'\n",
    "\n",
    "S -> N[BAR=1] V[BAR=2] \n",
    "N[BAR=1] -> N[BAR=0] \n",
    "V[BAR=2] -> V[BAR=1] N[BAR=1] \n",
    "V[BAR=1] -> V[BAR=0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  Modify the German grammar in Example 9-4 to incorporate the treatment of subcategorization presented in Section 9.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-109-67d7c1847e5c>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-109-67d7c1847e5c>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "% start S\n",
    " # Grammar Productions\n",
    " S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
    " NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
    " NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
    " VP[AGR=?a] -> IV[[AGR=?a], SUBCAT=intrans]\n",
    " VP[AGR=?a] -> TV[[OBJCASE=?c, AGR=?a], SUBCAT=trans] NP[CASE=?c]\n",
    " # Lexical Productions\n",
    " # Singular determiners\n",
    " # masc\n",
    " Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der'\n",
    " Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
    " Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
    " # fem\n",
    " Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
    " Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
    " Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
    " # Plural determiners\n",
    " Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die'\n",
    " Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den'\n",
    " Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die'\n",
    " # Nouns\n",
    " N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
    " N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    " N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
    " N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    " N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
    " N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
    " # Pronouns\n",
    " PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
    " PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
    " PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
    " PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
    " PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
    " PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
    " PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    " PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    " PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
    " PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
    " # Verbs\n",
    " IV[AGR=[NUM=sg,PER=1], SUBCAT=intrans] -> 'komme'\n",
    " IV[AGR=[NUM=sg,PER=2], SUBCAT=intrans] -> 'kommst'\n",
    " IV[AGR=[NUM=sg,PER=3], SUBCAT=intrans] -> 'kommt'\n",
    " IV[AGR=[NUM=pl, PER=1], SUBCAT=intrans] -> 'kommen'\n",
    " IV[AGR=[NUM=pl, PER=2], SUBCAT=intrans] -> 'kommt'\n",
    " IV[AGR=[NUM=pl, PER=3], SUBCAT=intrans] -> 'kommen'\n",
    " TV[OBJCASE=acc, AGR=[NUM=sg,PER=1], SUBCAT=trans] -> 'sehe' | 'mag'\n",
    " TV[OBJCASE=acc, AGR=[NUM=sg,PER=2], SUBCAT=trans] -> 'siehst' | 'magst'\n",
    " TV[OBJCASE=acc, AGR=[NUM=sg,PER=3], SUBCAT=trans] -> 'sieht' | 'mag'\n",
    " TV[OBJCASE=dat, AGR=[NUM=sg,PER=1], SUBCAT=trans] -> 'folge' | 'helfe'\n",
    " TV[OBJCASE=dat, AGR=[NUM=sg,PER=2], SUBCAT=trans] -> 'folgst' | 'hilfst'\n",
    " TV[OBJCASE=dat, AGR=[NUM=sg,PER=3], SUBCAT=trans] -> 'folgt' | 'hilft'\n",
    " TV[OBJCASE=acc, AGR=[NUM=pl,PER=1], SUBCAT=trans] -> 'sehen' | 'moegen'\n",
    " TV[OBJCASE=acc, AGR=[NUM=pl,PER=2], SUBCAT=trans] -> 'sieht' | 'moegt'\n",
    " TV[OBJCASE=acc, AGR=[NUM=pl,PER=3], SUBCAT=trans] -> 'sehen' | 'moegen'\n",
    " TV[OBJCASE=dat, AGR=[NUM=pl,PER=1], SUBCAT=trans] -> 'folgen' | 'helfen'\n",
    " TV[OBJCASE=dat, AGR=[NUM=pl,PER=2], SUBCAT=trans] -> 'folgt' | 'helft'\n",
    " TV[OBJCASE=dat, AGR=[NUM=pl,PER=3], SUBCAT=trans] -> 'folgen' | 'helfen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Develop a feature-based grammar that will correctly describe the following Spanish noun phrases:\n",
    "(59) un cuadro hermos-o\n",
    "INDEF.SG.MASC picture beautiful-SG.MASC\n",
    "‘a beautiful picture’\n",
    "(60) un-os cuadro-s hermos-os\n",
    "INDEF-PL.MASC picture-PL beautiful-PL.MASC\n",
    "‘beautiful pictures’\n",
    "\n",
    "(61) un-a cortina hermos-a\n",
    "INDEF-SG.FEM curtain beautiful-SG.FEM\n",
    "‘a beautiful curtain’\n",
    "(62) un-as cortina-s hermos-as\n",
    "INDEF-PL.FEM curtain beautiful-PL.FEM\n",
    "‘beautiful curtains’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_a = load_parser('file:Cp9_6b.fcfg')\n",
    "orac1 = 'un cuadro hermoso'.split()\n",
    "orac2 = 'unos cuadros hermosos'.split()\n",
    "orac3 = 'una cortina hermosa'.split()\n",
    "orac4 = 'unas cortinas hermosas'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP[]\n",
      "  (Det[ARG=[GND='MASC', NUM='pl']] unos)\n",
      "  (NP[ARG=[GND='MASC', NUM='pl']]\n",
      "    (N[ARG=[GND='MASC', NUM='pl']] cuadros)\n",
      "    (ADJ[ARG=[GND='MASC', NUM='pl']] hermosos)))\n"
     ]
    }
   ],
   "source": [
    "for tree in grammar_a.parse(orac2):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ◑ Develop a wrapper for the earley_parser so that a trace is only printed if the\n",
    "input sequence fails to parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. ◑ Consider the feature structures shown in Example 9-5.\n",
    "Example 9-5. Exploring feature structures.\n",
    "fs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\n",
    "fs2 = nltk.FeatStruct(\"[B = [D = d]]\")\n",
    "fs3 = nltk.FeatStruct(\"[B = [C = d]]\")\n",
    "fs4 = nltk.FeatStruct(\"[A = (1)[B = b], C->(1)]\")\n",
    "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
    "fs6 = nltk.FeatStruct(\"[A = [D = d]]\")\n",
    "fs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\n",
    "fs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\n",
    "fs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\n",
    "fs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -> (1)]\")\n",
    "Work out on paper what the result is of the following unifications. (Hint: you might find it useful to draw the graph structures.)\n",
    "a. fs1 and fs2\n",
    "b. fs1 and fs3\n",
    "c. fs4 and fs5\n",
    "d. fs5 and fs6\n",
    "e. fs5 and fs7\n",
    "f. fs8 and fs9\n",
    "g. fs8 and fs10\n",
    "Check your answers using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\n",
    "fs2 = nltk.FeatStruct(\"[B = [D = d]]\")\n",
    "fs3 = nltk.FeatStruct(\"[B = [C = d]]\")\n",
    "fs4 = nltk.FeatStruct(\"[A = (1)[B = b], C->(1)]\")\n",
    "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
    "fs6 = nltk.FeatStruct(\"[A = [D = d]]\")\n",
    "fs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\n",
    "fs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\n",
    "fs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\n",
    "fs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -> (1)]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         [ B = 'b'  ] ]\n",
      "[ A = (1) [ D = 'b'  ] ]\n",
      "[         [ E -> (1) ] ]\n",
      "[         [ G = 'b'  ] ]\n",
      "[                      ]\n",
      "[ C -> (1)             ]\n"
     ]
    }
   ],
   "source": [
    "print(fs8.unify(fs10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. ◑ Extend the German grammar in Example 9-4 so that it can handle so-called verbsecond\n",
    "structures like the following:\n",
    "(63) Heute sieht der Hund die Katze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. ◑ Seemingly synonymous verbs have slightly different syntactic properties (Levin,\n",
    "1993). Consider the following patterns of grammaticality for the verbs loaded,\n",
    "filled, and dumped. Can you write grammar productions to handle such data?\n",
    "(64) a. The farmer loaded the cart with sand\n",
    "b. The farmer loaded sand into the cart\n",
    "c. The farmer filled the cart with sand\n",
    "d. *The farmer filled sand into the cart\n",
    "e. *The farmer dumped the cart with sand\n",
    "f. The farmer dumped sand into the cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent12a = \"The farmer loaded the cart with sand\".lower().split()\n",
    "sent12b = \"The farmer loaded sand into the cart\".lower().split()\n",
    "sent12c = \"The farmer filled the cart with sand\".lower().split()\n",
    "sent12d = \"The farmer filled sand into the cart\".lower().split()\n",
    "sent12e = 'The farmer dumped the cart with sand'.lower().split()\n",
    "sent12f = 'The farmer dumped sand into the cart'.lower().split()\n",
    "grammar_12 = load_parser('file:Cp9_12d.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[NUM='sg'] (Det[NUM='sg'] the) (N[NUM='sg', +count] farmer))\n",
      "  (VP[AGR=?a]\n",
      "    (TV[OBJCASE='f2', TENSE='past'] dumped)\n",
      "    (NP[NUM='sg'] (Det[NUM='sg'] the) (N[NUM='sg', +count] cart))\n",
      "    (PP[NUM=?n] (P[OBJCASE='f1'] with) (N[NUM='sg', -count] sand))))\n"
     ]
    }
   ],
   "source": [
    "for tree in grammar_12.parse(sent12e):\n",
    "    print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
