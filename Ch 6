>>> import nltk
>>> import nltk.tokenize
>>> from nltk.tokenize import sent_tokenize, word_tokenize
>>> from nltk.corpus import brown
>>> from nltk.corpus import names
>>> import random


#2. Using any of the three classifiers described in this chapter, and any features you can think of, build the best name gender classifier you can. 
Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. 
Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress.
Once you are satisfied with your classifier, check its final performance on the test set. How does the performance on the test set compare to the performance on the dev-test set? 
Is this what you'd expect?

names = ([(name, 'male') for name in names.words('male.txt')] +
         [(name, 'female') for name in names.words('female.txt')])    
>>> random.shuffle(names)
>>> def gender_features(word):
        return {'last_letter': word[-1],
		'length': len(word),
		'first letter': word[0].lower(),
		'sec_letter':word[:2],
		'last_letter2':word[-2:],
		'last_letter3':word[-3:]}

>>> featuresets = [(gender_features2(n), gender) for (n, gender) in names]
>>> train_names = names[6900:]
>>> devtest_names = names[500:6900]
>>> test_names = names[:500]
>>> train_set = [(gender_features(n), gender) for (n, gender) in train_names]
>>> devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]
>>> test_set = [(gender_features(n), gender) for (n, gender) in test_names]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> print(nltk.classify.accuracy(classifier, devtest_set))
0.78890625
