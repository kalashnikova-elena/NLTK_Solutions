{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№1. Define a string s = 'colorless'. Write a Python statement that changes this to “colourless” using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s1 = s[:4] + 'u' + s[4:]\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№2 We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we’ve inserted a hyphen to indicate\n",
    "the affix boundary, but omit this from your strings): dish-es, run-ning, nationality, un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish run nation do heat\n"
     ]
    }
   ],
   "source": [
    "a = 'dishes'[:-2]\n",
    "b = 'running'[:-4]\n",
    "c = 'nationality'[:-5]\n",
    "d = 'undo'[2:]\n",
    "e = 'preheat'[3:]\n",
    "print(a,b,c,d,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№3 We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№4 We can specify a “step” size for the slice. The following returns every second character within the slice: monty[6:11:2]. It also works in the reverse direction:\n",
    "monty[10:5:-2]. Try these for yourself, and then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tni'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'nationality'[2:11:3]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№5 5. What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ytnom'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'monty'[::-1]\n",
    "a\n",
    "# [:] is the string itself, and :-1 takes the reverse order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№6 Describe the class of strings matched by the following regular expressions:\n",
    "a. [a-zA-Z]+  #Any alphabetical caracter  from small a to small z or capital A to capital Z, at least one character\n",
    "b. [A-Z][a-z]* # capital A to capital Z, small a to small z, 0 or more occurrences of any character.\n",
    "c. p[aeiou]{,2}t #p followed by no more than two vowels, followed by t\n",
    "d. \\d+(\\.\\d+)? # Real numbers(integers and fractions)\n",
    "e. ([^aeiou][aeiou][^aeiou])* # Consonant-Vowel-Consonant, zero or more times\n",
    "f. \\w+|[^\\w\\s]+  # matches alphanumeric character(s) or non-whitespace character(s),\n",
    "Test your answers using nltk.re_show()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№7 Write regular expressions to match the following classes of strings:\n",
    "a. A single determiner (assume that a, an, and the are the only determiners)\n",
    "b. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r'(^an?$|^the$)'\n",
    "r'(\\d\\*\\d\\+\\d)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№8 Write a utility function that takes a URL as its argument, and returns the contents\n",
    "of the URL, with all HTML markup removed. Use urllib.urlopen to access the contents of the URL, e.g.:\n",
    "raw_contents = urllib.urlopen('http://www.nltk.org/').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nNatural Language Toolkit — NLTK 3.5 documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNLTK 3.5 documentation\\n\\nnext |\\n          modules |\\n          index\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNatural Language Toolkit¶\\nNLTK is a leading platform for building Python programs to work with human language data.\\nIt provides easy-to-use interfaces to over 50 corpora and lexical\\nresources such as WordNet,\\nalong with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\\nwrappers for industrial-strength NLP libraries,\\nand an active discussion forum.\\nThanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\\nNLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\\nNLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\\nNLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\\nand “an amazing library to play with natural language.”\\nNatural Language Processing with Python provides a practical\\nintroduction to programming for language processing.\\nWritten by the creators of NLTK, it guides the reader through the fundamentals\\nof writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\\nand more.\\nThe online version of the book has been been updated for Python 3 and NLTK 3.\\n(The original Python 2 version is still available at http://nltk.org/book_1ed.)\\n\\nSome simple things you can do with NLTK¶\\nTokenize and tag some text:\\n>>> import nltk\\n>>> sentence = \"\"\"At eight o\\'clock on Thursday morning\\n... Arthur didn\\'t feel very good.\"\"\"\\n>>> tokens = nltk.word_tokenize(sentence)\\n>>> tokens\\n[\\'At\\', \\'eight\\', \"o\\'clock\", \\'on\\', \\'Thursday\\', \\'morning\\',\\n\\'Arthur\\', \\'did\\', \"n\\'t\", \\'feel\\', \\'very\\', \\'good\\', \\'.\\']\\n>>> tagged = nltk.pos_tag(tokens)\\n>>> tagged[0:6]\\n[(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'),\\n(\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\')]\\n\\n\\nIdentify named entities:\\n>>> entities = nltk.chunk.ne_chunk(tagged)\\n>>> entities\\nTree(\\'S\\', [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'),\\n           (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\'),\\n       Tree(\\'PERSON\\', [(\\'Arthur\\', \\'NNP\\')]),\\n           (\\'did\\', \\'VBD\\'), (\"n\\'t\", \\'RB\\'), (\\'feel\\', \\'VB\\'),\\n           (\\'very\\', \\'RB\\'), (\\'good\\', \\'JJ\\'), (\\'.\\', \\'.\\')])\\n\\n\\nDisplay a parse tree:\\n>>> from nltk.corpus import treebank\\n>>> t = treebank.parsed_sents(\\'wsj_0001.mrg\\')[0]\\n>>> t.draw()\\n\\n\\n\\nNB. If you publish work that uses NLTK, please cite the NLTK book as\\nfollows:\\n\\nBird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O’Reilly Media Inc.\\n\\n\\n\\nNext Steps¶\\n\\nsign up for release announcements\\njoin in the discussion\\n\\n\\n\\n\\nContents¶\\n\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\n\\nIndex\\nModule Index\\nSearch Page\\n\\n\\n\\n\\n\\n\\n\\nTable of Contents\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnext |\\n            modules |\\n            index\\n\\n\\n\\nShow Source\\n\\n\\n\\n\\n        © Copyright 2020, NLTK Project.\\n      Last updated on Apr 13, 2020.\\n      Created using Sphinx 2.4.4.\\n    \\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request \n",
    "raw = request.urlopen('http://nltk.org/').read().decode('utf8')\n",
    "raw = BeautifulSoup(raw, 'html.parser').get_text()\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№ 9. Save some text into a file corpus.txt. Define a function load(f) that reads from\n",
    "the file named in its sole argument, and returns a string containing the text of the\n",
    "file.\n",
    "a. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various\n",
    "kinds of punctuation in this text. Use one multiline regular expression inline\n",
    "comments, using the verbose flag (?x).\n",
    "b. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following\n",
    "kinds of expressions: monetary amounts; dates; names of people and\n",
    "organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "def load(file):\n",
    "    f = open(file).read()\n",
    "    return f\n",
    "load('C:\\Python38\\corpus.txt')\n",
    "\n",
    "pattern = r'''(?x)         # set flag to allow verbose regexps #re.VERBOSE : This flag allows you to write regular expressions that look nicer and are more readable by allowing you to visually separate logical sections of the pattern and add comments.\n",
    "    [,\\.]                 # comma, period #\\S any non-whitespace character\n",
    "    | [\\[\\](){}<>]          # brackets () {} [] <>\n",
    "    | ['\"“]                 # quotation marks\n",
    "    | [?!]                  # question mark and exclamation mark\n",
    "    | [:;]                  # colon and semicolon\n",
    "    (/\\./)\n",
    "    '''\n",
    "print(nltk.regexp_tokenize(file, pattern))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The United States dollar (sign: $;(2.23$) or($2.38) code: USD; also abbreviated US$ and referred to as the dollar, U.S. dollar, or American dollar) is the official currency of the United States and its territories per the Coinage Act of 1792. One dollar is divided into 100 cents (symbol: ?) or 1000 mills (for accounting purposes and for taxing; symbol: ?). The Coinage Act of 1792 created a decimal currency by creating the following coins: tenth dollar, one-twentieth dollar, one-hundredth dollar. In addition the act created the dollar, half dollar, and quarter dollar coins. All of these coins are still minted in 2020.\\n\\nIn addition, several forms of paper money were introduced by Congress over the years. The latest of these, the Federal Reserve Note, was authorized by the Federal Reserve Act of 1913 (3 May 1913), while all existing U.S. currency remains legal tender.[5] Issuance of the previous form of the currency (U.S. notes) was discontinued in January 1971.[6] As a result, currently circulating paper money consists primarily of Federal Reserve Notes that are denominated in United States dollars'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(file):\n",
    "    f = open(file).read()\n",
    "    return f\n",
    "load('C:\\Python38\\corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def load(file):\n",
    "    f = open(file).read()\n",
    "    return f\n",
    "load('C:\\Python38\\corpus.txt') \n",
    "\n",
    "pattern = r'''(?x)        # set flag to allow verbose regexps\n",
    "        (\\d+\\.)?\\d+\\s?\\$\n",
    "        | \\d{4}-\\d{2}-\\d{2}\n",
    "        | \\d{1,2}\\s[A-Z][a-z]*\\s\\d{4}\n",
    "        | (?:[A-Z][a-z]\\)+)\n",
    "    '''\n",
    "print(nltk.regexp_tokenize(file, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№10. Rewrite the following loop as a list comprehension:\n",
    ">>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    ">>> result = []\n",
    ">>> for word in sent:\n",
    "... word_len = (word, len(word))\n",
    "... result.append(word_len)\n",
    ">>> result\n",
    "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "print([(word, len(word)) for word in sent])                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "№11. Define a string raw containing a sentence of your own choosing. Now, split raw on some character other than space, such as 's'. (\n",
    "s.split(t)\tsplit s into a list wherever a t is found (whitespace by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Define a \", 'tring raw containing a ', 'entence of your own choo', \"ing'\"]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"'Define a string raw containing a sentence of your own choosing'\"\"\"\n",
    "raw.split('s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№ 12. Write a for loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n",
      "e\n",
      "f\n",
      "i\n",
      "n\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "raw1 = 'Define'\n",
    "for w in raw1:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№13. What is the difference between calling split on a string with no argument and\n",
    "one with ' ' as the argument, e.g., sent.split() versus sent.split(' ')? What\n",
    "happens when the string being split contains tab characters, consecutive space\n",
    "characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to\n",
    "enter a tab character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Define', 'a', 'string', 'raw']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'Define a string raw'\n",
    "raw.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Define', 'a', 'string', 'raw']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Define', 'a', 'string', 'raw.']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw2 = 'Define\\ta\\tstring\\traw.'\n",
    "raw2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Define\\ta\\tstring\\traw.']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw2.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Define', 'a', 'string', 'raw']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw3 = 'Define  a  string  raw  '\n",
    "raw3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Define', '', 'a', '', 'string', '', 'raw', '', '']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw3.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Define', 'a', 'string', 'raw.']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw4 = 'Define\\t a\\t string\\t raw.'\n",
    "raw4.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Define\\t', 'a\\t', 'string\\t', 'raw.']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw4.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№14. Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'containing', 'list', 'of', 'words', 'words']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = \"words containing a list of words\".split()\n",
    "words.sort()\n",
    "sorted(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words.sort() modifies the original variable, but will not output in default.\n",
    "#sorted(words) returns a sorted list without changing the original list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№15. Explore the difference between strings and integers by typing the following at a\n",
    "Python prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers\n",
    "using int(\"3\") and str(3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"3\" * 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"3\") * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(3) * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№17. What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a\n",
      "containing\n",
      "  list\n",
      "    of\n",
      " words\n",
      " words\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print ('%6s' %w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a     \n",
      "containing\n",
      "list  \n",
      "of    \n",
      "words \n",
      "words \n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print ('%-6s' %w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The % operator in python for strings is used for something called string substitution. String and Unicode objects have one unique built-in operation: the % operator (modulo).\n",
    "print 'I am %d years old'%14\n",
    "I am 14 years old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№18 Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses, and exclamations: who, which, what, and so on.) Print them in order. Are any words\n",
    "duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'when', 'Why', 'white', 'When', 'wh-question-word-song.htm', 'WHy', 'white-space', 'what', 'Where', 'WH', 'Which', 'whose', 'whom', 'wh-question-words-quiz.php', 'Whom', 'where', 'who', 'WH-', 'why', 'Whose', 'which', 'What', 'Who'}\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.englishclub.com/vocabulary/wh-question-words.htm'\n",
    "file = request.urlopen(url).read().decode('utf8')\n",
    "tokens = word_tokenize(file)\n",
    "print(set([wh for wh in tokens if wh.lower().startswith('wh')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№19  Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g., fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line\n",
    "into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('C:\\Python38\\In1.txt').readlines()\n",
    "fields = []\n",
    "for line in lines:\n",
    "    field = line.split()\n",
    "    field[1] = int(field[1])\n",
    "    fields.append(field)\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№20  Write code to access a favorite web page and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№21. Write a function unknown() that takes a URL as its argument, and returns a list  of unknown words that occur on that web page. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any\n",
    "items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['selfies', 'definitionselfiespictures', 'raped', 'takes', 'pictures', 'themself', 'anothers', 'cameradude', 'margaret', 'raped', 'selfies', 'mrmong', 'selfies', 'has', 'vaccinated', 'brags', 'weeks', 'vaxhole', 'selfies', 'vaxhole', 'girlsholds', 'mateby', 'selfy', 'thoby', 'usally', 'girls', 'involves', 'photos', 'ones', 'usally', 'selfies', 'facebook', 'cunt', 'facebook', 'hottest', 'selfies', 'bobwilllong', 'selfies', 'instagramby', 'ones', 'selfy', 'trending', 'carrots', 'sputnik', 'guvy', 'knockin', 'fridge', 'obnoxion', 'edward', 'hands', 'heels', 'columbus', 'ain', 'georgia', 'terms', 'serviceprivacy', 'dmcahelp', 'blog', 'terms', 'serviceprivacy', 'dmcahelp', 'blog']\n"
     ]
    }
   ],
   "source": [
    "def unknown(url):\n",
    "    file = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(file, 'html.parser').get_text()\n",
    "    lowercase_l = re.findall(r'\\b[a-z]+',raw) \n",
    "    known_words = nltk.corpus.words.words()\n",
    "    unknown_w = [w for w in lowercase_l if w not in known_words] #set(tokens).difference(wordlist).\n",
    "    print(unknown_w)\n",
    "url = 'https://www.urbandictionary.com/define.php?term=selfies'\n",
    "unknown(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word forms, neologisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№22. Examine the results of processing the URL http://news.bbc.co.uk/ using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly JavaScript commands. You may also find that\n",
    "sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№23. Are you able to write a regular expression to tokenize text in such a way that the word don’t is tokenized into do and n’t? Explain why this regular expression won’t work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'able', 'to', 'write', 'a', 'regular', 'expression', 'to', 'tokenize', 'text', 'in', 'such', 'a', 'way', 'that', 'the', 'word', 'don', 't', 'is', 'tokenized', 'into', 'do', 'and', 'n', 't']\n"
     ]
    }
   ],
   "source": [
    "raw = 'Are you able to write a regular expression to tokenize text in such a way that the word don’t is tokenized into do and n’t?'\n",
    "#print (re.findall(r\"n't|\\w+\", raw))\n",
    "print (re.findall(r\"\\w+(?=n\\'t)|n\\'t|\\w+\", raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№24 Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u51ng r3gu|ar 3xpr35510n5 and 5ub5t1tut10n'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"UsiNg regular eXpressions and suBSTitution\"\n",
    "def convert(text):\n",
    "    text = text.lower()\n",
    "    cnvrt = [('ate', '8'), ('e', '3'), ('i', '1'), ('o', '0'), ('l', '|'), ('s', '5'), ('\\.', '5w33t!')]\n",
    "    for (key, value) in cnvrt:\n",
    "            text = re.sub(key, value, text)\n",
    "    return text\n",
    "convert(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№26 Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ya', 'ye', 'ye', 'ja', 'ye', 'ye', 'yi', 'ai', 'ye', 'aj', 'ye', 'ya', 'ye', 'ye', 'ai', 'ye', 'ya', 'ye']\n",
      "   a  e  i  j \n",
      "a  0  0  2  1 \n",
      "j  1  0  0  0 \n",
      "y  3 10  1  0 \n"
     ]
    }
   ],
   "source": [
    "hung = \"A magyar nyelv az uráli nyelvcsalád tagja, a finnugor nyelvek közé tartozó ugor nyelvek egyike. Legközelebbi rokonai a manysi és a hanti nyelv, majd utánuk az udmurt, a komi, a mari és a mordvin nyelvek. Vannak olyan vélemények, melyek szerint a moldvai csángó önálló nyelv – különösen annak északi, középkori változata –, így ez volna a magyar legközelebbi rokonnyelve\"\n",
    "vow_seq = r\"[aeiouyj]{2,}\" \n",
    "pieces = re.findall(vow_seq, hung)\n",
    "print(pieces)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(pieces)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№27 Python's random module includes a function choice() which randomly chooses an item from a sequence, e.g. choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he  haha ee  heheeh eha. Use split() and join() again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aah hehh eh hahh ehe ehahh heh ha ah e eh hehhh e h e hahhh e ha haehhehhh heeah hah he hehaehhhe a eh h eeheeh ehhahahhhehhee h h hahaeah aeha haheahhheheahheaa ee aaheaahhaaeehe hhh hhhee hhhahaehaehahh hhhehaah hhaea ehhhaheh ehhehheah ehheh hhehhhheaeeeaahaeeheeehhaeeh ahhe aeah a ehhhh aeh eehhahehahheheah hehehehheehheeheeaeee haae h haha eah h h h ha hhaehaa eaheeh he e ehhhhaahaahheheheehh ae eeh heh a e e hhehaahhhaaaeeh hh aaea hheaahhh ahehae e aahaaa ahhhe hhh a a'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "sequence = \"aehh \"\n",
    "n= 500\n",
    "def z():\n",
    "    xx = ''.join(random.choice(sequence) for s in range(n))\n",
    "    return ' '.join(xx.split())\n",
    "        \n",
    "z()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№28. Consider the numeric expressions in the following sentence from the MedLine Corpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53 +/- 0.15% is three words? Or should we say that it's a single compound word? Or should we say that it is actually nine words, since it's read \"four point five three, plus or minus zero point fifteen percent\"? Or should we say that it's not a \"real\" word at all, since it wouldn't appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for building natural language understanding applications numbers should be recognized semantically as one compound word (one enry)\n",
    "# for building speech processing applications numbers should be recognized as 11 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№29. Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for various sections of the Brown Corpus, including section f (lore) and j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.254756197101155\n",
      "11.926007043317348\n"
     ]
    }
   ],
   "source": [
    "def ari(category):\n",
    "    words = nltk.corpus.brown.words(categories=category)\n",
    "    sents = nltk.corpus.brown.sents(categories=category)\n",
    "    μw = sum(len(w) for w in words) / len(words)\n",
    "    μs = sum(len(s) for s in sents) / len(sents)\n",
    "    ari = 4.71 * μw + 0.5 * μs - 21.43\n",
    "    print(ari)\n",
    "\n",
    "ari('lore')\n",
    "ari('learned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№30. Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer, and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', 'the', 'porter', 'stemmer', 'to', 'normal', 'some', 'token', 'text', ',', 'call', 'the', 'stemmer', 'on', 'each', 'word', '.', 'Do', 'the', 'same', 'thing', 'with', 'the', 'lancast', 'stemmer', ',', 'and', 'see', 'if', 'you', 'observ', 'ani', 'differ', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = 'Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer, and see if you observe any differences.'\n",
    "tokens = word_tokenize(raw)\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['us', 'the', 'port', 'stem', 'to', 'norm', 'som', 'tok', 'text', ',', 'cal', 'the', 'stem', 'on', 'each', 'word', '.', 'do', 'the', 'sam', 'thing', 'with', 'the', 'lancast', 'stem', ',', 'and', 'see', 'if', 'you', 'observ', 'any', 'diff', '.']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancaster stemmer produces many mistakes: use, some, normal, token, call, any, differ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№31. Define the variable saying to contain the list ['After', 'all', 'is', 'said',　'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']. Process the list using a for loop, and store the result in a new list lengths. Hint: begin by assigning　the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "list_length = []\n",
    "for s in saying:\n",
    "    list_length.append(len(s))\n",
    "list_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№32. Define a variable silly to contain the string: 'newly formed bland ideas are inexpressible in an infuriating way'. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky’s famous\n",
    "nonsense phrase colorless green ideas sleep furiously, according to Wikipedia). Now write code to perform the following tasks:\n",
    "a. Split silly into a list of strings, one per word, using Python’s split() operation, and save this to a variable called bland.\n",
    "b. Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.\n",
    "c. Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace.\n",
    "d. Print the words of silly in alphabetical order, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eoldrnnnna\n",
      "newly formed bland ideas are inexpressible in an infuriating way\n",
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "bland = silly.split()\n",
    "second = []\n",
    "for b in bland:\n",
    "    second.append(b[1]) #''.join(w[1] for w in bland)\n",
    "print(''.join(second))\n",
    "print(' '.join(bland))\n",
    "for b in sorted(bland):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№33. The index() function can be used to look up items in sequences. For example, 'inexpressible'.index('e') tells us the index of the first position of the letter e.\n",
    "a. What happens when you look up a substring, e.g., 'inexpressible'.index('re')?\n",
    "b. Define a variable words containing a list of words. Now use words.index() to look up the position of an individual word.\n",
    "Define a variable silly as in the exercise above. Use the index() function in combination with list slicing to build a list phrase consisting of all the words up to (but not including) in in silly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'inexpressible'.index('re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tells the index of the position of 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "words.index('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tells the index of the first occurence of 'is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are '"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "silly[:silly.index('in')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№34. Write code to convert nationality adjectives such as Canadian and Australian to their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/ List_of_adjectival_forms_of_place_names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Canada'"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nat_adj(adj):\n",
    "    if adj.endswith('ian'):\n",
    "        return adj[:-3] + 'a'\n",
    "\n",
    "nat_adj('Canadian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№35. Read the LanguageLog post on phrases of the form as best as p can and as best p can, where p is a pronoun. Investigate this phenomenon with the help of a corpus\n",
    "and the findall() method for searching tokenized text described in Section 3.5.\n",
    "The post is at http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = request.urlopen('https://thegrammarexchange.infopop.cc/topic/as-best-you-can').read().decode('utf8')\n",
    "text = BeautifulSoup(raw, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As best you can',\n",
       " 'As best you can',\n",
       " 'As best you can',\n",
       " 'As best you can',\n",
       " 'As best you can',\n",
       " 'as best I can',\n",
       " 'as best you can',\n",
       " 'as best I can',\n",
       " 'As best you can']"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(?i)as best (?:as )?(?:I|we|you|he|she|they|it|one) can', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№36. Study the lolcat version of the book of Genesis, accessible as nltk.corpus.genesis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expressions\n",
    "to convert English words into corresponding lolspeak words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№37. Read about the re.sub() function for string substitution using regular expressions, using help(re.sub) and by consulting the further readings for this chapter.\n",
    "Use re.sub in writing code to remove HTML tags from an HTML file, and to normalize whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№38 An interesting challenge for tokenization is words that have been split across a\n",
    "linebreak. E.g., if long-term is split, then we have the string long-\\nterm.\n",
    "a. Write a regular expression that identifies words that are hyphenated at a linebreak.\n",
    "The expression will need to include the \\n character.\n",
    "b. Use re.sub() to remove the \\n character from these words.\n",
    "c. How might you identify words that should not remain hyphenated once the\n",
    "newline is removed, e.g., 'encyclo-\\npedia'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long-\\nterm', 'encyclo-\\npedia']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'long-term is split, then we have the string long-term, encyclo-pedia'"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'long-term is split, then we have the string long-\\nterm, encyclo-\\npedia'\n",
    "split_string = re.findall(r'\\w+-\\n\\w+', text)\n",
    "print(split_string)\n",
    "re.sub('\\n', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long-\n",
      "term\n",
      "encyclo-\n",
      "pedia\n"
     ]
    }
   ],
   "source": [
    "#c ???\n",
    "for w in split_string:\n",
    "    if w not in nltk.corpus.words.words():\n",
    "        print(w) #(re.sub('\\-', '', split_string))\n",
    "    else:\n",
    "        print(split_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№39 Read the Wikipedia entry on Soundex. Implement this algorithm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A0'"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def soundex(word):\n",
    "    soundex_word = word.upper()\n",
    "    soundex_word = word[0]\n",
    "    soundex_word = re.sub(r'a|e|i|o|u|y|h|w', '', soundex_word) #(r'[bfpv]', '1', word)\n",
    "    soundex_word = re.sub(r'b|f|p|v', '1', soundex_word)\n",
    "    soundex_word = re.sub(r'c|g|j|k|q|s|x|z', '2', soundex_word)\n",
    "    soundex_word = re.sub(r'd|t', '3', soundex_word)\n",
    "    soundex_word = re.sub(r'l', '4', soundex_word)\n",
    "    soundex_word = re.sub(r'm|n', '5', soundex_word)\n",
    "    soundex_word = re.sub(r'r', '6', soundex_word)\n",
    "#??? If two or more letters with the same number are adjacent in the original name \n",
    "#also two letters with the same number separated by 'h' or 'w' are coded as a single number. This rule also applies to the first letter.\n",
    "#4.retain only the first three number.\n",
    "    if len(soundex_word) < 3:\n",
    "        soundex_word += '0'\n",
    "\n",
    "    return soundex_word[:4]\n",
    "soundex('Ashcraft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№40 Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g., compare ABC Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sentence\n",
    "segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rural.txt', 'science.txt']"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.abc.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ...]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = nltk.corpus.abc.sents('rural.txt')\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.348730626705681\n"
     ]
    }
   ],
   "source": [
    "def ari(fileid):\n",
    "    #sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    words = nltk.corpus.abc.words(fileids=fileid)\n",
    "    sents = nltk.corpus.abc.sents(fileids=fileid)\n",
    "    \n",
    "    μw = sum(len(w) for w in words) / len(words)\n",
    "    μs = sum(len(s) for s in sents) / len(sents)\n",
    "    ari = 4.71 * μw + 0.5 * μs - 21.43\n",
    "    print(ari)\n",
    "ari('rural.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№41 Rewrite the following nested loop as a nested list comprehension:\n",
    " words = ['attribution', 'confabulation', 'elocution',\n",
    "... 'sequoia', 'tenacious', 'unidirectional']\n",
    " vsequences = set()\n",
    " for word in words:\n",
    "vowels = []\n",
    "for char in word:\n",
    "if char in 'aeiou':\n",
    "vowels.append(char)\n",
    "vsequences.add(''.join(vowels))\n",
    "sorted(vsequences)\n",
    "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']\n",
    "vsequences = [''.join(re.findall(r'[aeiou]', vowels)) for vowels in words]\n",
    "sorted(vsequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "№42 Use WordNet to create a semantic index for a text collection. Extend the concordance\n",
    "search program in Example 3-1, indexing each word using the offset of\n",
    "its first synset, e.g., wn.synsets('dog')[0].offset (and optionally the offset of some\n",
    "of its ancestors in the hypernym hierarchy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№43 With the help of a multilingual corpus such as the Universal Declaration of\n",
    "Human Rights Corpus (nltk.corpus.udhr), along with NLTK’s frequency distribution\n",
    "and rank correlation functionality (nltk.FreqDist, nltk.spearman_correla\n",
    "tion), develop a system that guesses the language of a previously unseen text. For\n",
    "simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№44Write a program that processes a text and discovers cases where a word has been\n",
    "used with a novel sense. For each word, compute the WordNet similarity between\n",
    "all synsets of the word and all synsets of the words in its context. (Note that this\n",
    "is a crude approach; doing it well is a difficult, open research problem.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№45 Read the article on normalization of non-standard words (Sproat et al., 2001),\n",
    "and implement a similar system for text normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
