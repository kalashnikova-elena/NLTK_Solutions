>>> import nltk
>>> import nltk.tokenize
>>> from nltk.tokenize import sent_tokenize, word_tokenize
>>> from nltk.corpus import brown

#1 Search the web for "spoof newspaper headlines", to find such gems as: British Left Waffles on Falkland Islands, and Juvenile Court to Try Shooting Defendant. 
#Manually tag these headlines to see if knowledge of the part-of-speech tags removes the ambiguity.

>>> text = word_tokenize('British Left Waffles on Falkland Islands')
>>> nltk.pos_tag(text)
[('British', 'JJ'), ('Left', 'NNP'), ('Waffles', 'NNP'), ('on', 'IN'), ('Falkland', 'NNP'), ('Islands', 'NNP')]

#"left waffles" - "a vague talk of the Left wing" ambiguity reduced

#3 Tokenize and tag the following sentence: They wind back the clock, while we chase after the wind. What different pronunciations and parts of speech are involved?

>>> text = word_tokenize('They wind back the clock, while we chase after the wind')
>>> nltk.pos_tag(text)
[('They', 'PRP'), ('wind', 'VBP'), ('back', 'RB'), ('the', 'DT'), ('clock', 'NN'), (',', ','), ('while', 'IN'), ('we', 'PRP'), ('chase', 'VBP'), ('after', 'IN'), ('the', 'DT'), ('wind', 'NN')]

# 'wind' - 'VBP' and 'NN'; 'chase'- 'VBP'

#5. Using the Python interpreter in interactive mode, experiment with the dictionary examples in this chapter. Create a dictionary d, and add some entries. What happens
whether you try to access a non-existent entry, e.g., d['xyz']?

#6 Try deleting an element from a dictionary d, using the syntax del d['abc']. Check that the item was deleted.
>>> d = {}
>>> d['abc'] = 'A'
>>> d['bcd'] = 'B'
>>> d['cde'] = 'C'
>>> d
{'abc': 'A', 'bcd': 'B', 'cde': 'C'}
>>> del d['abc']
>>> d
{'bcd': 'B', 'cde': 'C'}
>>> d['xyz']
Traceback (most recent call last):
  File "<pyshell#18>", line 1, in <module>
    d['xyz']
KeyError: 'xyz'

#7 Create two dictionaries, d1 and d2, and add some entries to each. Now issue the command d1.update(d2). What did this do? What might it be useful for?
>>> d2 = {}
>>> d2['aa'] = 'AA'
>>> d2['bb'] = 'BB'
>>> d2['cde'] = 'CC'
>>> d.update(d2)
>>> d
{'bcd': 'B', 'cde': 'CC', 'aa': 'AA', 'bb': 'BB'}

#update() method adds element(s) to the dictionary if the key is not in the dictionary. If the key is in the dictionary, it updates the key with the new value.
# You can add a new key-value pair, update multiple key value pairs at a time.

#10 Train a unigram tagger and run it on some new text. Observe that some words are not assigned a tag. Why not?
>>> brown_tagged_sents = brown.tagged_sents(categories='news')
>>> brown_sents = brown.sents(categories='news')
>>> unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
>>>  tt = 'What is the upper limit to the performance of an n-gram tagger'
>>> tokens = tt.split()
>>> unigram_tagger.tag(tokens)
[('What', 'WDT'), ('is', 'BEZ'), ('the', 'AT'), ('upper', 'JJ'), ('limit', 'NN'), ('to', 'TO'), ('the', 'AT'), ('performance', 'NN'), ('of', 'IN'), ('an', 'AT'), ('n-gram', None), ('tagger', None)]

#new words, that were not among the trained data were assigned "None" tag

#11 Learn about the affix tagger (type help(nltk.AffixTagger)). Train an affix tagger and run it on some new text. Experiment with different settings for the affix length
and the minimum word length. Discuss your findings. 
>>> affix_tagger = nltk.AffixTagger(brown_tagged_sents, affix_length=-2, min_stem_length=3)
>>> text = 'See if you can come up with patterns to improve the performance of the above regular expression tagger'.split()
>>> affix_tagger.tag(text)
[('See', None), ('if', None), ('you', None), ('can', None), ('come', None), ('up', None), ('with', None), ('patterns', 'NNS'), ('to', None), ('improve', 'JJ'), ('the', None), ('performance', 'NN'), ('of', None), ('the', None), ('above', 'JJ'), ('regular', 'JJ'), ('expression', 'NN'), ('tagger', 'NN')]
>>> affix_tagger = nltk.AffixTagger(brown_tagged_sents, affix_length=-2, min_stem_length=3)
>>> affix_tagger.tag(text)
[('See', None), ('if', None), ('you', None), ('can', None), ('come', None), ('up', None), ('with', None), ('patterns', 'NNS-TL'), ('to', None), ('improve', 'VB'), ('the', None), ('performance', 'NN'), ('of', None), ('the', None), ('above', None), ('regular', 'JJ'), ('expression', 'NN'), ('tagger', 'NN')]

#12. Train a bigram tagger with no backoff tagger, and run it on some of the training data. Next, run it on some new data. What happens to the performance of the tagger? Why?
#Separating the Training and Testing Data:
>>> size = int(len(brown_tagged_sents) * 0.9)
>>> train_sents = brown_tagged_sents[:size]
>>> test_sents = brown_tagged_sents[size:]

>>> bigram_tagger = nltk.BigramTagger(train_sents)
>>> bigram_tagger.tag(brown_sents[2009])
[('The', 'AT'), ('structures', 'NNS'), ('housing', 'VBG'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('masonry', 'NN'), ('and', 'CC'), ('frame', 'NN'), ('construction', 'NN'), ('.', '.')]
>>> new_sent = 'See if you can come up with patterns to improve the performance of the above regular expression tagger'.split()
>>> bigram_tagger.tag(new_sent)
[('See', None), ('if', None), ('you', None), ('can', None), ('come', None), ('up', None), ('with', None), ('patterns', None), ('to', None), ('improve', None), ('the', None), ('performance', None), ('of', None), ('the', None), ('above', None), ('regular', None), ('expression', None), ('tagger', None)]
# there were no such words pairs in the train data and the bigram tagger cannot identify the words it saw in combination with words tagged as 'None'

#14. Use sorted() and set() to get a sorted list of tags used in the Brown corpus, removing duplicates.
>>> list1 = sorted(set([tag for (_, tag) in brown.tagged_words()]))
>>> list1
["'", "''", '(', '(-HL', ')', ')-HL', '*', '*-HL', '*-NC', '*-TL', ',', ',-HL', ',-NC', ',-TL', '--', '---HL', '.', '.-HL', '.-NC', '.-TL', ':', ':-HL', ':-TL', 'ABL', 'ABN', 'ABN-HL', 'ABN-NC', 'ABN-TL', 'ABX', 'AP', 'AP$', 'AP+AP-NC', 'AP-HL'

#15. Write programs to process the Brown Corpus and find answers to the following questions:

b. Which word has the greatest number of distinct tags? What are they, and what do they represent?

a. Which nouns are more common in their plural form, rather than their singular
form? ???(Only consider regular plurals, formed with the -s suffix.)
>>> brown_tagged_words = brown.tagged_words()
>>> cfd = nltk.ConditionalFreqDist(brown_tagged_words)
>>> pl_list = [w for w in cfd.conditions() if cfd[w]['NNS'] > cfd[w]['NN']]

c. List tags in order of decreasing frequency. What do the 20 most frequent tags represent?
>>> list2 = [tag for (_, tag) in brown.tagged_words()]
>>> fd = nltk.FreqDist(list2)
>>> fd.most_common(20)
[('NN', 152470), ('IN', 120557), ('AT', 97959), ('JJ', 64028), ('.', 60638), (',', 58156), ('NNS', 55110), ('CC', 37718), ('RB', 36464), ('NP', 34476), ('VB', 33693), ('VBN', 29186), ('VBD', 26167), ('CS', 22143), ('PPS', 18253), ('VBG', 17893), ('PP$', 16872), ('TO', 14918), ('PPSS', 13802), ('CD', 13510)]

d. Which tags are nouns most commonly found after? What do these tags represent? (done in book)
	
>>> word_tag_pairs = nltk.bigrams(brown_news_tagged)
>>> noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'NOUN']
>>> fdist = nltk.FreqDist(noun_preceders)
>>> [tag for (tag, _) in fdist.most_common()]
['NOUN', 'DET', 'ADJ', 'ADP', '.', 'VERB', 'CONJ', 'NUM', 'ADV', 'PRT', 'PRON', 'X']
>>> sorted(set(noun_followers))
['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']

#16. Explore the following issues that arise in connection with the lookup tagger:
a. What happens to the tagger performance for the various model sizes when a backoff tagger is omitted?
b. Consider the curve in Figure 5-4; suggest a good size for a lookup tagger that balances memory and performance. Can you come up with scenarios where it
would be preferable to minimize memory usage, or to maximize performance with no regard for memory usage?

The tagger performance will increase with the increase of the model size. In order to balance memory and performance, use a 90% performance = 8000-9000 model size. 

#17. What is the upper limit of performance for a lookup tagger, assuming no limit to the size of its table? 
(Hint: write a program to work out what percentage of tokens of a word are assigned the most likely tag for that word, on average.)

>>> brown_tagged_sents = brown.tagged_sents(categories='news')
>>> fd = nltk.FreqDist(brown.words(categories='news'))
>>> cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))
>>> likely_tags = dict((word, cfd[word].max()) for word in fd.keys())
>>> baseline_tagger = nltk.UnigramTagger(model=likely_tags)
>>> baseline_tagger.evaluate(brown_tagged_sents)
0.9349006503968017

#18. Generate some statistics for tagged data to answer the following questions:
What proportion of word types are always assigned the same part-of-speech tag?
How many words are ambiguous, in the sense that they appear with at least two tags?
What percentage of word tokens in the Brown Corpus involve these ambiguous words?

>>> corpus = brown.words(categories='news')
>>> cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))
>>> same_pos = [w for w in cfd.conditions() if len(cfd[w]) == 1]
>>> print(len(same_pos)/len(cfd.conditions()))
0.8802973461164374
>>> ambi_pos = [w for w in cfd.conditions() if len(cfd[w]) > 1]
>>> print(len(ambi_pos)
1723
>>> tokens = [w for w in corpus if w in ambi_pos]
>>> print(len(tokens)/len(corpus))
0.5808421345744575

#19. The evaluate() method works out how accurately the tagger performs on this text. For example, if the supplied tagged text was [('the', 'DT'), ('dog', 'NN')] and the tagger produced the output [('the', 'NN'), ('dog', 'NN')] , then the score would be 0.5 . Let's try to figure out how the evaluation method works:
a. A tagger t takes a list of words as input, and produces a list of tagged words as output. However, t.evaluate() is given correctly tagged text as its only parameter. What must it do with this input before performing the tagging?
the input should be tagged with the tagging model
b.Once the tagger has created newly tagged text, how might the evaluate() method go about comparing it with the original tagged text and computing the accuracy score?
the tagger model is campared with the correctly tagged text:the number of correct predictions made is devided by the total number of predictions made

20. Write code to search the Brown Corpus for particular words and phrases according to tags, to answer the following questions:
a. Produce an alphabetically sorted list of the distinct words tagged as MD. 
b. Identify words that can be plural nouns or third person singular verbs (e.g. deals, flies).
c. Identify three-word prepositional phrases of the form IN + DET + NN (eg. in the lab).
What is the ratio of masculine to feminine pronouns?

a.
>>> wsj = nltk.corpus.treebank.tagged_words()
>>> cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in wsj)
>>> list(sorted(cfd2['MD']))
["'d", "'ll", 'Can', 'Could', 'ca', 'can', 'could', 'may', 'might', 'must', 'ought', 'shall', 'should', 'will', 'wo', 'would']

b.
>>> wsj = nltk.corpus.treebank.tagged_words()
>>> cfd1 = nltk.ConditionalFreqDist(wsj)
>>> [w for w in cfd1.conditions() if 'NN2' in cfd1[w] and 'VVZ' in cfd1[w]]

c. 
>>> def prep_phrase(sentence):
	for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence):
		if (t1 == ('IN') and t2.startswith('A') and t3.startswith('N')):
			print(w1, w2, w3)

			
>>> for tagged_sent in brown.tagged_sents():
	prep_phrase(tagged_sent)
	
of the election
of the City
for the manner
in the election
to the end

d.  masc = ['he', 'him', 'his', 'himself']
>>> fem = ['she', 'her', 'hers', 'herself']
>>> masc_list = [w for w in brown.words() if w.lower() in masc]
>>> fem_list = [w for w in brown.words() if w.lower() in fem]
>>> print(len(masc_list)/len(fem_list))
3.2741427861520624

#21 In Table 3-1, we saw a table involving frequency counts for the verbs adore, love, like, and prefer, and preceding qualifiers such as really. Investigate the full range
of qualifiers (Brown tag QL) that appear before these four verbs. 
>>> brown_tagged_words = brown.tagged_words()
>>> bigrams = nltk.bigrams(brown_tagged_words)
>>> verbs = ['adore', 'love', 'prefer', 'like']
>>> ql = set([w1[0] for w1, w2 in bigrams if w2[0] in verbs and w1[1] == 'QL'])
>>> print(ql)

#22


